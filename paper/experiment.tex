\section{Experiment}
\label{s:experiment}



\subsection{Dataset}

\XXX{plot simhash of known cloaking}

Because we want to measure cloaking in SEO and SEM, we have compiled a list of
cloaking words, from the policies specified by Adwords, inspired by the words
collection process in ~\cite{wang2011cloak}. We have looked at the policies, and
collected \XXX{N} words, from ad network abuse, adult abuse, alcohol abusive, dangerous behavior
abuse, dishonest behavior abuse, health abuse, gambling abuse.

Step 1: For each word, with chrome user agent, we click and visit search results and advertisements on first
twenty pages. \\
Step 2: For the landing pages collected in step 1, with google bot user agent, we visit them 6 times at an
interval of around 20 miniutes.

6 copy is collected because we need to model the distribution of websites. It is
a tradeoff between space and precision. If we want to model the website better,
we may want to collect as many examples as possible.


\subsection{Groundtruth}

We remove the failure websites and this results in 
113242 urls, exact match, parameter different are counted.
98390 sites, parameters ignored. Later we will use the latter parameter because
it makes more sense.


Visit \XXX{N} search words, for collected search urls, we have collected
\XXX{130K} URLs.


\bf{Step 1: Filter}

In order to get groundtruth, we follow a similar process employed in
~\cite{lin2009detection}, we first filter the results and get rid of the highly reputated ones. We write a
script to query the WOT API, and remove websites with combined score 80
(which is a pretty high score) and the results are \XXX{N} urls after that.

for advertisements, after filtering, there are 2279 (score 60) urls
remained.\XXX{Problematic because I haven't merged them}

for search results, after filtering, there are 90120 (60) urls remained.
\XXX{Problematic because I haven't merged them}


\bf{Step 2: De-duplication}
This results in \XXX{Some} links. Then we compare the text simhash and dom
simhash, remove those which are exactly duplicate of one of the simhash observed
by Google. After this step, we have \XXX{N} url left.

For advertisements, after deduplication, there are 997 (score 60) urls remained.

For search results, after deduplication 37155 urls, 35444 websites remained.


\bf{Step 3: Random Sample and Labeling}


Then we randomly select 1000 urls from the dataset, and label them, after
labeling, we found \XXX{N} cloaking sites and \XXX{M} dynamic websites.
These are the groundtruth we used to label our data.

\subsection{Detection and Evaluation}
We have detected.

We manually examine the results and found \XXX{N} are actually cloaking.

For SEO and SEM, the cloaking rate is 5\% and 3\% in the dataset collected by
us.




We see a lower clokaing rate compared to XXX, may be because Google has done
something to this. However, this problem still remains.
