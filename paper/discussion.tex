\section{Discussion}
\label{s:discussion}

In this part, we will discuss two kinds of deployment of simhash based cloaking detection system: server-based and crowdsoucing.

\subsection{Server-based cloaking detetion system}
In server-based cloaking detection system
we first collect targeted search terms includes commecial terms, cloaking oriented terms and hot tread words.
Using these search terms, the cralwer extracts the search results from the search engines for seven times.
In the six times, the crawler disguises as Google Crawlers by using the Google Agent. With these
data that is crawled from Google view, the system uses the simhash method to model the websites. The last
time, the crawler disguises as normal users using User Agent. The system compares simhash value from user view and
website model learned from Google view. From the comparsion result, the system judge is the website is cloaking
or not. 
Server based cloaking detection system has pros and cons. This deployment is pratical and could be deployed
easily. The tradeoff of easy deployment is inefficient in IP cloaking detection. Usually, the servers IP addresses
are in a range. Scammers could find this range and serve benign content to crawlers. One solution is buying numerous IP addresses
from ISP providers and distribute IP addresses as users distribution. This increases cloaking detection cost. In
addition, distributed IP addresses as users distribution is hard. In addition, server-based cloaking detection system is infeasiable
to detect cloaking in search engine marketing(SEM). As we mentioned, using the crawlers to visit websites in SEM field
will increase the advertisement cost of websites. Further, to save compuation resources, the system need to find different 
crawling periods for different websites. 


Pros:	practical, easy deployment.
cons:	IP cloaking, SEM, hard to decide crawling periods. 
Solution: IP cloaking, Buy 100k IP.
SEM, Deduce visits by search engine. 

\subsection{Crowdsourcing}
Crowdsourcing cloaking detection system includes user-side and server-side component. In user side, users needs to install the cloaking
detection extension in their browers. While users click search results and view websites, the extention calculate the simhash value from
website content. After calculation, extension packs URL and simhash value and sends to server. Server passively receives (URL, simhash value)
 pairs from users. Server also utilizes crawlers to extract content several times in this URL from search engine view (crawler uses Google bot agents).
 Server uses these extract content to model the website. From the comparison result between (URL, simhash value) from users and website model,
 server could decide if the website is cloaking or not. From previous study, cloaking includes phishing and malware downloading. Based on cloaking
 detection result, Server categorizes cloaking and update blacklists in browsers' extensions. Updating blacklists and warning users phishing and
 malware downloading is users incentive to install our extension. 

 Previously, we describe our crowdsoucing workflow. Next, We discuss the pros and cons for this approach. The first advantage is privacy. Instead of solicting
 website content from users, the system solicited a 64 bits simhash value from users. From this 64 bits value, system couldn't do reverse engineering to
 know what user sees. In addition, the system could intergrate with Rappor~\cite{erlingsson2014rappor}, which protect the privacy of URL. The system couldn't
 know the users' url while not influencinng system functions. In addition, the crowdsouring deployment is low traffic. Browsers only  send a 64 bits value for
 each URL. This won't jam traffic. Further, the crowdsoucing deployment wouldn't affect the model of SEM. Each click on advertisements of search engines is
 from real users instead of crawlers. Advertisers don't need to pay extra money for cloaking detection. 


Employ RAPPOR~\cite{erlingsson2014rappor} to provide user privacy gurantee.

pros: 	1.privacy 2.Low traffic. 3.SEM 4.Distributed computation 
5. Remove the need to do redirect cloaking detection, leveraging the feature
that the end goal of attackers is to reach user
6. could decide crawl period passively based on user clicks, data received are
based on real userâ€™s clicks, say, website traffic

cons: user incentives. 
Solution: Plugin to detect suspicious websites. API


\begin{figure}[t]
  \centering
  \includegraphics[width=.5\textwidth]{fig/workflow}
  \caption{Workflow of crowdsource cloaking detection sysetem}
  \label{fig:workflow}
\end{figure}


The workflow ~\autoref{fig:workflow} is to collect page contents simhash on the user side, and compare
them to simhash of the same link from ad serving company to find cloaking. When
the differences of the simhashes are significantly large, the page is marked
cloaking. We generate two simhash for page content and structure respectively.
Intuition behind this is, simhash difference between different sites are larger
than different visits of the same site. We build a two-phase system to detect
cloaking: cluster learning phase, and cloaking detection phase. In the cluster
learning phase, an ad company visit urls and generate simhash from its content
with its owned IP, and learn pattern and distribution of the simhashes, i.e.
simhash-based website model. In the cloaking detection phase, the ad company
collects simhash from its users. Compare them with learned patterns, return
cloaking score or mismatch

