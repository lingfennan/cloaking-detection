\section{Discussion}
\label{s:discussion}

In this part, we will discuss two kinds of deployment of simhash based cloaking detection system: server based and crowdsoucing.

\subsection{Server-based cloaking detetion system}
In server-based cloaking detection system
we first collect targeted search terms includes commecial terms, cloaking oriented terms and hot tread words.
Using these search terms, the cralwer extracts the search results from the search engines for seven times.
In the six times, the crawler disguises as Google Crawlers by using the Google Agent. With these
data that is crawled from Google view, the system uses the simhash method to model the websites. The last
time, the crawler disguises as normal users using User Agent. The system compares simhash value from user view and
website model learned from Google view. From the comparsion result, the system judge is the website is cloaking
or not. 
Server based cloaking detection system has pros and cons. This deployment is pratical and could be deployed
easily. The tradeoff of easy deployment is inefficient in IP cloaking detection. Usually, the servers IP addresses
are in a range. Scammers could find this range and serve benign content to crawlers. One solution is buying numerous IP addresses
from ISP providers and distribute IP addresses as users distribution. This increases cloaking detection cost. In
addition, distributed IP addresses as users distribution is hard. In addition, server-based cloaking detection system is infeasiable
to detect cloaking in search engine marketing(SEM). As we mentioned, using the crawlers to visit websites in SEM field
will increase the advertisement cost of websites. Further, to save compuation resources, the system need to find different 
crawling periods for different websites. 


Pros:	practical, easy deployment.
cons:	IP cloaking, SEM, hard to decide crawling periods. 
Solution: IP cloaking, Buy 100k IP.
SEM, Deduce visits by search engine. 

\subsection{Crowdsource based}
Employ RAPPOR~\cite{erlingsson2014rappor} to provide user privacy gurantee.

pros: 	1.privacy 2.Low traffic. 3.SEM 4.Distributed computation 
5. Remove the need to do redirect cloaking detection, leveraging the feature
that the end goal of attackers is to reach user
6. could decide crawl period passively based on user clicks, data received are
based on real userâ€™s clicks, say, website traffic

cons: user incentives. 
Solution: Plugin to detect suspicious websites. API


\begin{figure}[t]
  \centering
  \includegraphics[width=.5\textwidth]{fig/workflow}
  \caption{Workflow of crowdsource cloaking detection sysetem}
  \label{fig:workflow}
\end{figure}


The workflow ~\autoref{fig:workflow} is to collect page contents simhash on the user side, and compare
them to simhash of the same link from ad serving company to find cloaking. When
the differences of the simhashes are significantly large, the page is marked
cloaking. We generate two simhash for page content and structure respectively.
Intuition behind this is, simhash difference between different sites are larger
than different visits of the same site. We build a two-phase system to detect
cloaking: cluster learning phase, and cloaking detection phase. In the cluster
learning phase, an ad company visit urls and generate simhash from its content
with its owned IP, and learn pattern and distribution of the simhashes, i.e.
simhash-based website model. In the cloaking detection phase, the ad company
collects simhash from its users. Compare them with learned patterns, return
cloaking score or mismatch

