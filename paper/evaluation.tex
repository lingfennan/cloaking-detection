\section{Evaluation}
\label{s:evaluation}



\subsection{Dataset}

\XXX{plot simhash of known cloaking}

Because we want to measure cloaking in SEO and SEM, we have compiled a list of
cloaking words, from the policies specified by Adwords, inspired by the words
collection process in ~\cite{wang2011cloak}. We have looked at the policies, and
collected \XXX{N} words, from ad network abuse, adult abuse, alcohol abusive, dangerous behavior
abuse, dishonest behavior abuse, health abuse, gambling abuse.

Step 1: For each word, with chrome user agent, we click and visit search results and advertisements on first
twenty pages.
Step 2: For the landing pages collected in step 1, with google bot user agent, we visit them 6 times at an
interval of around 20 miniutes.

6 copy is collected because we need to model the distribution of websites. It is
a tradeoff between space and precision. If we want to model the website better,
we may want to collect as many examples as possible.


We randomly sample 600 websites from the dataset, for 10 times. This results in
5726 websites. We manually label them and \XXX{cloaking}, \XXX{not}, percentage
for each is.


\subsection{Groundtruth}

We remove the failure websites and this results in 
113242 urls, exact match, parameter different are counted.
98390 sites, parameters ignored. Later we will use the latter parameter because
it makes more sense.


Visit 1000 search words, for collected search urls, we have collected
130K URLs.
Maybe not

%Step 1: Filter
%
%In order to get groundtruth, we follow a similar process employed in
%~\cite{lin2009detection}, we first filter the results and get rid of the highly reputated ones. We write a
%script to query the WOT API, and remove websites with combined score 80
%(which is a pretty high score) and the results are \XXX{N} urls after that.
%
%for advertisements, after filtering, there are 2279 (score 60) urls
%remained.\XXX{Problematic because I haven't merged them}
%
%for search results, after filtering, there are 90120 (60) urls remained.
%\XXX{Problematic because I haven't merged them}
%

\subsubsection{De-duplication}
71116 urls

62042 websites

This results in \XXX{Some} links. Then we compare the text simhash and dom
simhash, remove those which are exactly duplicate of one of the simhash observed
by Google. After this step, we have \XXX{N} url left.

For advertisements, after deduplication, there are 997 (score 60) urls remained.

For search results, after deduplication 37155 urls, 35444 websites remained.

\subsubsection{Random Sample and Labeling}
Then we randomly select 1000 urls from the dataset, and label them, after
labeling, we found \XXX{N} cloaking sites and \XXX{M} dynamic websites.
These are the groundtruth we used to label our data.

\subsection{Detection and Evaluation}
We have detected.
We manually examine the results and found \XXX{N} are actually cloaking.
For SEO and SEM, the cloaking rate is 5\% and 3\% in the dataset collected by
us.

We see a lower cloaking rate compared to XXX, may be because Google has done
something to this. However, this problem still remains.

For the United States, web search dataset.

The advertisements are
before dedup: 4381
after dedup: 1487
\subsection{Cross Domain Spam Detection}
In our detection result, we have observed many cloaking cases, where URL are
completely different, while the content are similar or even the same.
We argue that, the foundamental reason for them to do this is the low cost of
getting a new URL and lack of efficient way to detect spammy content.
Based on this observation, we propose content based blacklist to raise the bar
for reusing spammy content. This approach leverages the most popular use of
simhash - near duplicate detection. For spammy pages, in order to evade our
detection, they not only need to change URL rapidly, but also update their
content everytime, which can be expensive in their current mode if they want
every copy their website to be different and have meaningful and stay attractive
to user.

