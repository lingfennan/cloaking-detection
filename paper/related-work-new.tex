\section{Background}
\label{s:related-work}
Search engine, as a most innovative technology introduced in late 20th century, has widely influenced our relationships. Search engine used their own page ranking algorthim to rank and indexed websites. In this way, users could find information by entering the search terms into the search enginee. To get better rankings and accurate indexed on search engines such as Google, websites are encouraged by Google to optimize their content. This is a technology called search engine optimization (SEO).
SEO policies were introduced by search engine later to maintain a fairness of search enginee ranking. Recently, some websites break the SEO policies to get better rankings on websites because of the large profits. These technology are called Black Hat SEO, which is used to get better rankings on search engine but break the SEO policy.
From the SEO policy, one of the most efficient methods to get better page rankings is to update the content of website frequently~\cite{wang2011cloak}.
Following this rule, cloaking, a Black Hat SEO method, is invented. Cloaking serves a blatently different information to users and Google to maintain their
profits and get better page rank. To be clear, we give a general example. Websites provide a frequently updated information to the Google so that they could get
better rank. On the other side, they send a different information to users to get profits.

\subsection{Example}
%To concrete the idea of cloaking, a specific cloaking example, malicious downlowd cloaking, is introduced. We entered the term "loose slot machines las vegas"
%in Google. Google returned a set of search results as showned in figure 1.
%From the figure 1, we see that the top 6 search results are all under domain "dje,.com.ol". If we disguised ourselves as normal users without setting HTTP proxy 
%agent and clicked the links, the broswers sent an HTTP request with Google reference. The website received our HTTP request and found Google reference. The website
%redirected several times, eventually landed to a page and automatically download malicious software. On the other hand, we reconfiguerd our browsers and set
%Google-Bot as our HTTP agent. We revisited the website. This time, the website provide us totally different information without redirecting and malicious downloading.
%From two scenarios, the cloaking idea is clear. Websites provide legal information to Google to get good page rank and avoid censorships. On the other side,
%websites provide different information with redericting and malicious downloads to get profits from users. 


To concrete the idea of cloaking, a specific cloaking example, traffic sale 
cloaking, is introduced. We entered the term "instant loan quote"
in Google. Google returned a set of search results as showned in
~\autoref{seo:search}, and the third result is doing cloaking. 
When clicking this link directly, ~\autoref{seo:user} is presented. However, if
we visit the landing page as spider (set user agent to Google bot),
~\autoref{seo:google} is shown.
%However, the content is actually from ~\autoref{seo:victim}. 
The fact is that, this website is doing blackhat SEO,
raising its rank with junk content, but when user clicks, they simply iframe
another page, monentizing from its ranking and the traffic.

%\subsection{Cloaking and Redirection}
%~\cite{wu2005cloaking} classifies 
%The landing page is different. The redirection.
%
%We use a different approach. We first click on the landing page, record the URL,
%and then disguise ourselves to visit that landing page. We leverage the fact
%that, if the cloakers employ some complex infrastructure to redirect users to
%different landing pages, and they are doing bad things. They will do user agent
%cloaking on the landing page as well. Otherwise, if search engine visits.
%
%This example: user goes to 'sh.st/pZ78x'. Google visit the same clickstring is
%redirected to '
%http://apktribe.com/games-andriod/slot-machines-space-of-a-laugh-las-vegas-on-line-casino-video-games-loose-spin-win-slots-roulette/'.
%However, if Google visit 'sh.st/pZ78x', Google is redirected back to
%'http://apktribe.com/games-andriod/slot-machines-space-of-a-laugh-las-vegas-on-line-casino-video-games-loose-spin-win-slots-roulette/'
%


\subsection{Search Engine Marketing}
Priviously, we briefly talked about search engine optimization. In this section, we introduce another area called search engine marking(SEM)
where enourmous cloaking websites exist. According to wikipedia~\cite{sem-wiki}, the term SEM is used to mean pay per click advertising,
particularly in the commercial advertising and marketing communities which have a vested interest in this narrow definition. To be clear,
search egnine marketing defines the advertisements shown on search engines after searching terms. Accoding to wikipedia~\cite{sem-wiki},
boundary beween SEO and SEM are sometimes not clear. However, when studying the cloaking, the SEO and SEM are clearly different. The difference
betwen them is because that the policies and rules on SEO and SEM are different. On SEM, the policy are usually strict and commertial-related. 
Further, the cloaking-incentives on SEO and SEM are different. On SEM, websites are more likely to provide illegal medicines and services. On SEO,
websites are inclined to do malicious downloading and phishing. 
Further, the strategy on cloaking detection on SEO and SEM are different. The difference is crucially decided by ranking mechanisims.
In SEO, though the page rank algorithm is not fully public, it is well known that rankings are related to content, page rank and visit traffic.
In SEM, the rank algroithm depends on real time biding system. Websites need to bid the "pay per click" price on the real time biding system. 
The one with the the higher price will be shown in higher priority. The terms with large commerical potential ask higher price. Because of the 
difference between SEM and SEO, the strategy to detect cloaking should change. The cloaking detection algorithm should change such as collecting commercial related terms, 
crawling adverstisements, checking the SEM policy(Google Ads Policy). To ensure "Pay per click" mechanisms, in other words,
websites won't pay much extra money because of cloaking detection, we introduced a new model and "click counting" mechanisms.
Traditional methods failed to do this adjust on SEM. Thus, the traditional methods are inefficient detect cloakings in SEM.
\subsection{Cloaking Types}
To understand how cloaking works in different scenarios, we will discuss the cloaking types.
In order to serve targeted users cloaking content, the scammer must use some identifiers to distinguish 
user segments. Based on these used identifiers, the cloaking techniques are classified as Repeat Cloaking, 
User Agent Cloaking, Referred Cloaking and IP Cloaking. 

In Repeat Cloaking, websites store the visit history in user-end(Cookies) or server-end (server log). Based on visit history, the website presents different information. 
According to ~\cite{wang2011cloak}, they oberved websites that only show cloaking at first time, in the hopes of making a sale, but subsequent visits are presented 
with benign page. In our observation, some repeat cloaking websites are willing to 
show the same content as the first time. 
In User Agent Cloaking, websites check the User Agent Field in the HTTP request. From the User agent field, they could find the crawlers that uses the well-known User-Agent
strings and identify crawlers.
In Referrer Cloaking, websites examamine the referer field of HTTP request. From the referer field, the website could easily find if the users clicked though search engines to
reach their websites. In this way, the cloaking websites only serves the scam page to the targeted users from search engines. 
In IP Cloaking, websites determine the visitors' identities by their IP addresses. With an accurate mapping between IP addresses
and ornizations, the websites could easily distinguish cralwers from search engines and real users. 


\subsection{Previous Work}
\subsubsection{Cloaking Detection}
The major challenge in claoking detection, is to differentiate dynamic pages
from blatantly different content.
Comparing document word by word is very expensive and slow, therefore, 
various ways to test similarity of documents are proposed. 
~\cite{henzinger2002challenges}
 considered cloaking as one of the major search engine spam
techniques.
 ~\cite{najork2005system} proposed a method of detecting cloaked pages from browsers by
 installing a toolbar. The toolbar would send the signature of user perceived
 pages to search engines.
 ~\cite{wu2006detecting} use statistics of web pages to detect cloaking,
 % common terms
~\cite{chellapilla2006improving} detected syntactic on the most popular and
 monetizable search terms. They showed that monetized search terms had a higher
 prevalence of cloaking than popular terms.
 Referrer cloaking was first studied by ~\cite{wang2006detecting}. They found
 a large number of referrer cloaking pages in their work.
 ~\cite{lin2009detection} use tag based methods,

 ~\cite{wang2011cloak}
 extended their previous efforts to examine the dynamics of cloaking over five
 months, identifying when distinct results were provided to search engine
 crawlers and browsers.
They used text-based method and tag-based method to detect Cloaking page.
 ~\cite{deng2013uncovering} use summarize previous work and compare text, tag,
 link based approaches. However, JavaScript redirects were not able be handled by their crawler.

However, non of them take into consideration the data collection part. Nor do
they take into account the efficiency of the algorithms.

In order to detect cloaking, we introduce Simhash-based Website Model (SWM) and
design and implement a much more efficient algorithm based on SWM, which has
comparable false positive rate and true positive rate. We take into account the
efficiency of the algorithm and implement a plugin which introduce negligible
overhead to user. With crowdsourcing, we can solve cloaking at scale.

%Previous work on phishing. We are solving cloaking, which has a strong
%relationship with phishing sites.

\subsubsection{Simhash}
Charikar's simhash~\cite{charikar2002similarity} has been widely used in near
duplicate detection in search engine. ~\cite{henzinger2006finding}
conducted a large-scale evaluation of simhash against Broder's shingle-based
fingerprints~\cite{broder1997syntactic} in finding near-duplicate web pages.
A great advantage of using simhash over shingles is that it
requires relatively small-sized fingerprints.
For example, ~\cite{broder1997syntactic} requires 24 bytes per fingerprint.
In comparison, ~\cite{manku2007detecting} shows that for 8B web pages, 64-bit
fingerprints suffice.

These approaches are mainly focusing on using simhash to signature websites on
the Internet and comparing them all together to identify near duplicates.
In contrast, this work leverages the fact that content from same url is supposed
to be near duplicate and employs simhash to do outlier detection.

This work adopts the same simhash algorithm and setting as 
~\cite{manku2007detecting}, but with different method of extrating text
features. ~\cite{manku2007detecting} extracts text features from website with
standard IR techniques, and weigh each feature with inverse document frequency.
This requires extra information and introduces overhead if deployed in browser at user side.
In ~\autoref{s:methodology}, we describe our approach of extracting text features.
Inspired from previous cloaking work~\autoref{wang2011cloak} which compares both
text and dom tree information, we extract dom features as well.



