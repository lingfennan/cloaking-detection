\section{Related Work}
\label{s:related work}
Search engine, as a most innovative technology  introduced in late 20th century, has widely influenced our relationships. Search engine used their own page ranking algorthim to rank and indexed websites. In this way, users could find information by entering the search terms into the search enginee. To get better rankings and accurate indexed on search engines such as Google, websites are encouraged by Google to optimize their content. This is a technology called search engine optimization (SEO).
SEO policies were introduced by search engine later to maintain a fairness of search enginee ranking. Recently, some websites break the SEO policies to get better rankings on websites because of the large profits. These technology are called Black Hat SEO, which is used to get better rankings on search engine but break the SEO policy.
From the SEO policy, one of the most efficient methods to get better page rankings is to update the content of website frequently~\cite{wang2011cloak}.
Following this rule, cloaking, a Black Hat SEO method, is invented. Cloaking serves a blatently different information to users and Google to maintain their
profits and get better page rank. To be clear, we give a general example. Websites provide a frequently updated information to the Google so that they could get
better rank. On the other side, they send a different information to users to get profits.

\subsection{Example}
To concrete the idea of cloaking, a specific cloaking example, malicious downlowd cloaking, is introduced. We entered the term "Game
Texas Holdem Poker Online - Dj w" in Google. Google returned a set of search results as showned in figure 1.
From the figure 1, we see that the top 6 search results are all under domain "dje,.com.ol". If we disguised ourselves as normal users without setting HTTP proxy 
agent and clicked the links, the broswers sent an HTTP request with Google reference. The website received our HTTP request and found Google reference. The website
redirected several times, eventually landed to a page and automatically download malicious software. On the other hand, we reconfiguerd our browsers and set
Google-Bot as our HTTP agent. We revisited the website. This time, the website provide us totally different information without redirecting and malicious downloading.
From two scenarios, the cloaking idea is clear. Websites provide legal information to Google to get good page rank and avoid censorships. On the other side,
websites provide different information with redericting and malicious downloads to get profits from users. 

\subsection{Search Engine Marketing}
Priviously, we briefly talked about search engine optimization. In this section, we introduce another area called search engine marking(SEM)
where enourmous cloaking websites exist. According to wikipedia~\cite{sem-wiki}, the term SEM is used to mean pay per click advertising,
particularly in the commercial advertising and marketing communities which have a vested interest in this narrow definition. To be clear,
search egnine marketing defines the advertisements shown on search engines after searching terms. Accoding to wikipedia~\cite{sem-wiki},
boundary beween SEO and SEM are sometimes not clear. However, when studying the cloaking, the SEO and SEM are clearly different. The difference
betwen them is because that the policies and rules on SEO and SEM are different. On SEM, the policy are usually strict and commertial-related. 
Further, the cloaking-incentives on SEO and SEM are different. On SEM, websites are more likely to provide illegal medicines and services. On SEO,
websites are inclined to do malicious downloading and phishing. 
Further, the strategy on cloaking detection on SEO and SEM are different. The difference is crucially decided by ranking mechanisims.
In SEO, though the page rank algorithm is not fully public, it is well known that rankings are related to content, page rank and visit traffic.
In SEM, the rank algroithm depends on real time biding system. Websites need to bid the "pay per click" price on the real time biding system. 
The one with the the higher price will be shown in higher priority. The terms with large commerical potential ask higher price. Because of the 
difference between SEM and SEO, the strategy to detect cloaking should change. The cloaking detection algorithm should change such as collecting commercial related terms, 
crawling adverstisements, checking the SEM policy(Google Ads Policy). To ensure "Pay per click" mechanisms, in other words,
websites won't pay much extra money because of cloaking detection, we introduced a new model and "click counting" mechanisms.
Traditional methods failed to do this adjust on SEM. Thus, the traditional methods are inefficient detect cloakings in SEM.
\subsection{Cloaking Types}
A website could differentiate search engine crawler from user web browser, from
the following data:
\subsubsection{IP address}
Attackers blacklist a list of IP addresses and simply block their access.
\subsubsection{HTTP user-agent header}
Attackers identify bots by looking at user-agent, serve different content.
\subsubsection{HTTP referer header}
Referer tells landing page, where user is coming from, they may only allow
access for users with referer http://www.google.com.
\subsubsection{Cookie}
This is used as a complementary method to referer, in order to track whether
user is shown malicious content before. If so, then subsequent accesses are
bad as well. If not, set cookie and disallow access to bad stuff.

This is mainly observed on websites, that try to provide illegal services. The
intuition of doing this is to track past user visit, and provide further service
to the victim.
\subsubsection{Number of visit times}
The website shows malicious content to a specific IP only once.

\subsection{Previous Work}
\subsubsection{Cloaking Detection}
The major challenge in claoking detection, is to differentiate dynamic pages
from blatantly different content.
Comparing document word by word is very expensive and slow, therefore, 
various ways to test similarity of documents are proposed. 
~\cite{henzinger2002challenges}
 considered cloaking as one of the major search engine spam
techniques.
 ~\cite{najork2005system} proposed a method of detecting cloaked pages from browsers by
 installing a toolbar. The toolbar would send the signature of user perceived
 pages to search engines.
 ~\cite{wu2006detecting} use statistics of web pages to detect cloaking,
 % common terms
~\cite{chellapilla2006improving} detected syntactic on the most popular and
 monetizable search terms. They showed that monetized search terms had a higher
 prevalence of cloaking than popular terms.
 Referrer cloaking was first studied by ~\cite{wang2006detecting}. They found
 a large number of referrer cloaking pages in their work.
 ~\cite{lin2009detection} use tag based methods,

 ~\cite{wang2011cloak}
 extended their previous efforts to examine the dynamics of cloaking over five
 months, identifying when distinct results were provided to search engine
 crawlers and browsers.
They used text-based method and tag-based method to detect Cloaking page.
 ~\cite{deng2013uncovering} use summarize previous work and compare text, tag,
 link based approaches. However, JavaScript redirects were not able be handled by their crawler.

However, non of them take into consideration the data collection part. Nor do
they take into account the efficiency of the algorithms.

In order to detect cloaking, we design a much more efficient algorithm with
comparable false positive rate and true positive rate. We take into account the
efficiency of the algorithm and implement a plugin which introduce negligible
overhead to user. With crowdsourcing, we can solving cloaking at scale.
Besides, previous 

Previous work on cloaking detection.

Previous work on phishing. We are solving cloaking, which has a strong
relationship with phishing sites.

Previous work on simhash. ~\cite{manku2007detecting} employ simhash to detect
near duplicate at scale.

